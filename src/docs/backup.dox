/*! @class doc_bulk_durability

Bulk-loads are not commit-level durable, that is, the creation and
bulk-load of an object will not appear in the database log files.\  For
this reason, applications doing incremental backups after a full backup
should repeat the full backup step after doing a bulk-load to make the
bulk-load durable.\ In addition, incremental backups after a bulk-load
can cause recovery to report errors because there are log records that
apply to data files which don't appear in the backup.

*/

/*! @m_page{{c,java},backup,Backups}

WiredTiger cursors provide access to data from a variety of sources.
One of these sources is the list of files required to perform a backup
of the database.  The list may be the files required by all of the
objects in the database, or a subset of the objects in the database.

WiredTiger backups are "on-line" or "hot" backups, and applications may
continue to read and write the databases while a snapshot is taken.

@section backup_process Backup from an application

1. Open a cursor on the \c "backup:" data source, which begins the
   process of a backup.

2. Copy each file returned by the WT_CURSOR::next method to the backup
   location, for example, a different directory. Do not reuse backup
   locations unless all files have first been removed from them, in
   other words, remove any previous backup information before using a
   backup location.

3. Close the cursor; the cursor must not be closed until all of the
   files have been copied.

The directory into which the files are copied may subsequently be
specified as an directory to the ::wiredtiger_open function and
accessed as a WiredTiger database home.

Copying the database files for a backup does not require any special
alignment or block size (specifically, Linux or Windows filesystems that
do not support read/write isolation can be safely read for backups).

The database file may grow in size during the copy, and the file copy
should not consider that an error. Blocks appended to the file after the
copy starts can be safely ignored, that is, it is correct for the copy
to determine an initial size of the file and then copy that many bytes,
ignoring any bytes appended after the backup cursor was opened.

The cursor must not be closed until all of the files have been copied,
however, there is no requirement the files be copied in any order or in
any relationship to the WT_CURSOR::next calls, only that all files have
been copied before the cursor is closed.  For example, applications might
aggregate the file names from the cursor and then list the file names as
arguments to a file archiver such as the system tar utility.

During the period the backup cursor is open, database checkpoints can
be created, but no checkpoints can be deleted.  This may result in
significant file growth.  Additionally while the backup cursor is open
automatic log file archiving, even if enabled, will not reclaim any
log files.

Additionally, if a crash occurs during the period the backup cursor is
open and logging is disabled (in other words, when depending on
checkpoints for durability), then the system will be restored to the
most recent checkpoint prior to the opening of the backup cursor, even
if later database checkpoints were completed. <b>Note this exception to
WiredTiger's checkpoint durability guarantees.</b>

The following is a programmatic example of creating a full backup:

@snippet ex_all.c backup

In cases where the backup is desired for a checkpoint other than the
most recent, applications can discard all checkpoints subsequent to the
checkpoint they want using the WT_SESSION::checkpoint method.  For
example:

@snippet ex_all.c backup of a checkpoint

@section backup_util Backup from the command line

The @ref_single util_backup command may also be used to create backups:

@code
rm -rf /path/database.backup &&
    mkdir /path/database.backup &&
    wt -h /path/database.source backup /path/database.backup
@endcode

@section backup_o_direct Backup and O_DIRECT

Many Linux systems do not support mixing \c O_DIRECT and memory mapping
or normal I/O to the same file.   If \c O_DIRECT is configured for data
or log files on Linux systems (using the wiredtiger_open \c direct_io
configuration), any program used to copy files during backup should also
specify \c O_DIRECT when configuring its file access.  Likewise, when
\c O_DIRECT is not configured by the database application, programs
copying files should not configure \c O_DIRECT.

@section backup_incremental_log Incremental backup using log files

Once a backup has been done, it can be rolled forward incrementally by
adding log files to the backup copy. Adding log files to the copy
decreases potential data loss from switching to the copy, but increases
the recovery time necessary to switch to the copy.  To reset the
recovery time necessary to switch to the copy, perform a full backup of
the database.  For example, an application might do a full backup of the
database once a week during a quiet period, and then incrementally copy
new log files into the backup directory for the rest of the week.
Incremental backups may also save time when the tables are very large.

@copydoc doc_bulk_durability

By default, WiredTiger automatically removes log files no longer
required for recovery.  Applications wanting to use log files for
incremental backup must first disable automatic log file removal using
the \c log=(archive=false) configuration to ::wiredtiger_open.

The following is the procedure for incrementally backing up a database
and removing log files from the original database home:

1. Perform a full backup of the database (as described above).

2. Open a cursor on the \c "backup:" data source, configured with the
   \c "target=(\"log:\\")" target specified, which begins the process
   of an incremental backup.

3. Copy each log file returned by the WT_CURSOR::next method to the backup
   directory.  It is not an error to copy a log file which has been copied
   before, but care should be taken to ensure each log file is completely copied
   as the most recent log file may grow in size while being copied.

4. If all log files have been successfully copied, archive the log
   files by calling the WT_SESSION::truncate method with the URI
   <code>log:</code> and specifying the backup cursor as the start
   cursor to that method. (Note there is no requirement backups be
   coordinated with database checkpoints, however, an incremental backup
   will repeatedly copy the same files, and will not make additional log
   files available for archival, unless there was a checkpoint after the
   previous incremental backup.)

5. Close the backup cursor.

Steps 2-5 can be repeated any number of times before step 1 is repeated.
Full and incremental backups may be repeated as long as the backup
database directory has not been opened and recovery run.  Once recovery
has run in a backup directory, you can no longer back up to that
database directory.

An example of opening the backup data source for an incremental backup using log files:

@snippet ex_all.c incremental backup using log files

If incremental backup fails (for example, if the backup directory runs out of space), a full
backup should be performed.

Incremental backup using log files cannot be used in conjunction with incremental backup
using block transfer.

@section backup_incremental_block Incremental backup using block transfer

Once a backup has been done, it can be rolled forward incrementally by updating file blocks
in the backup copy.

The following is the procedure for incrementally backing up a database using block transfer
from the original database home:

1. Perform a full backup of the database (as described above), with the additional configuration
   \c incremental_preserve.

2. Save the name of the full backup's origin checkpoint for use in subsequent incremental backups
   (it's the \c WT_CURSOR::checkpoint attribute of the backup cursor used to perform the full
   backup).

3. Begin the incremental backup by opening a cursor on the \c "backup:" data source, configured
   with \c "incremental_checkpoint=name", where the checkpoint name is the name of the starting
   checkpoint from which the backup is being rolled forward.

4. For each file returned by the WT_CURSOR::next method, open a duplicate backup cursor on the
   file, configured with \c "incremental_file=name", where the file name is the key returned
   by the backup cursor.

5. The key format for the duplicate backup cursor is \c qq, representing a file offset and size
   pair. There is no associated value. For each offset/size returned by the duplicate backup
   cursor's WT_CURSOR::next method, read the block from the source database file and write the
   block to the backup database file, replacing the current backup file's contents. It is not
   an error for an offset/size pair to extend past the current end of the file, and any missing
   data should be ignored.

6. Close the duplicate backup cursor.

7. Save the name of the checkpoint to which the backup was rolled forward for use as an origin
   checkpoint in subsequent incremental backups (it's the \c WT_CURSOR::checkpoint attribute
   of the backup cursor used to perform the incremental backup).

8. Close the backup cursor.

Steps 3-8 can be repeated any number of times. Incremental backups may be repeated as long as
the backup database directory has not been opened and recovery run. Once recovery has run in
a backup directory, you can no longer back up to that database directory.

Each time the incremental backup is repeated, the origin checkpoint name should be updated to
a new value. If failure occurs at any point, the incremental backup may be repeated from the
same origin checkpoint name; once an incremental backup has been performed using an updated
origin checkpoint name, it will no longer be possible to perform an incremental backup using
a previous origin checkpoint name.

The following is a programmatic example of creating a full backup with the expectation of
performing future incremental backups:

@snippet ex_all.c full backup for incremental

An example of opening the backup data source for an incremental backup using block transfer:

@snippet ex_all.c incremental backup using block transfer

To support incremental backup using block transfer, WiredTiger must track block changes across
checkpoints which can require significant resources. The granularity of this tracking can be
configured using the \c incremental_granularity configuration to WT_SESSION::open_cursor when
opening the first full-backup cursor. A larger configuration reduces the amount of information
WiredTiger must retain by conflating multiple block changes.

If incremental backup fails (for example, if the backup directory runs out of space), a full
backup should be performed.

Incremental backup using block transfer cannot be used in conjunction with incremental backup
using log files.

*/
